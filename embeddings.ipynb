{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "import fasttext\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from scipy.spatial import distance\n",
    "import gensim\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import openpyxl\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.io.formats.excel\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully loaded\n",
      "CPU times: user 4.87 s, sys: 21.3 s, total: 26.1 s\n",
      "Wall time: 24min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_path = 'BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model = sent2vec.Sent2vecModel()\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    data = pd.read_excel(path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_name(data):\n",
    "    service_name = pd.DataFrame(data['SERVICE_NAME'])\n",
    "    return service_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standards_path():\n",
    "    paths = []\n",
    "    for file in os.listdir(\"Standards\"):\n",
    "        paths.append(os.path.join(\"Standards\", file))\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_service_type(data, types):\n",
    "    data = data[data['SERVICE_TYPE'].isin(types)]\n",
    "    data.reset_index(inplace=True,drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_standards(paths):\n",
    "    stds = []\n",
    "    for path in paths:\n",
    "        stds.append(pd.read_excel(path).drop_duplicates())\n",
    "    return stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_providers_path():\n",
    "    paths = []\n",
    "    for file in os.listdir(\"Providers\"):\n",
    "        paths.append(os.path.join(\"Providers\", file))\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(DF1,col_index): #DF1 is the DF to be preprocessed #col_index is the number of the describtion column (int)\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    uselessWords = ['intended','for','use','helps','to','provide','all','detection','of']\n",
    "    # Making A copy to prevent change to the original DF as it is needd subsequently\n",
    "    DF = DF1.copy()\n",
    "    # Insuring string type for the wanted col\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].astype(str)\n",
    "    # Removing all newline and replacing it with space\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].str.replace('\\n',' ')\n",
    "    #Replacing all non alphanumeric in both english and arabic by space\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "    #Separating camel case with space\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", x))\n",
    "    #lowercasing all describtions\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "    #removing stop words\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x:\" \".join (word for word in x.split() if word not in stopWords))\n",
    "    #removing useless words\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x:\" \".join(word for word in x.split() if word not in uselessWords))\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x:''.join(i for i in x if not i.isdigit()))\n",
    "    #In case a med dict present (referring the medical abbreviation to its origin) this line return the abbreviation to origin\n",
    "    ## DF_document.document = DF_document.document.apply(lambda x: \" \".join(med_dict(word) for word in x.split()) )\n",
    "    #Counting all Words that occur either in very low freq or very high and removing them (Threshhold must be configured)\n",
    "    ## freq = pd.Series(' '.join(DF[DF.columns[col_index]]).split()).value_counts()[:50]\n",
    "    ## DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: \" \".join(x for x in x.split() if  x not in freq.index))\n",
    "    ## freq = pd.Series(' '.join(DF[DF.columns[col_index]]).split()).value_counts()[-1034:]\n",
    "    ## DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: \" \".join(x for x in x.split() if x not in freq.index))\n",
    "    #Defining a stemmer and stemming all words in the description\n",
    "    #st = PorterStemmer()\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: \"\" if x.isdigit() == True else x)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(data):\n",
    "    emb = model.embed_sentences(data)\n",
    "    emb = pd.DataFrame(emb)\n",
    "    #print(emb)\n",
    "    return pd.DataFrame(emb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos(sent1,sent2):\n",
    "    cosine_sim = 1 - distance.cosine(sent1, sent2)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_similar(sent1,emb_ref):\n",
    "#     similarities = []\n",
    "#     for i in range(len(emb_ref)):\n",
    "#         similarities.append(get_cos(sent1.values,emb_ref.iloc[i].values))\n",
    "#     #similarities = np.array(similarities)\n",
    "#     maximum = max(similarities)\n",
    "#     return similarities.index(maximum), maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(tfidf_data,tfidf_ref, index, rank = 1):\n",
    "    #multiplying the 2 matrix to find the cosine similarity\n",
    "    cosine_similarities = cosine_similarity(tfidf_data[index:index+1], tfidf_ref[:]).flatten()\n",
    "    #getting the most similar index\n",
    "    highest_ind = cosine_similarities.argsort()[::-1][rank]\n",
    "    #return the index along with its similarity\n",
    "    return highest_ind, cosine_similarities[highest_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_excel('Abeer Supreme Medical Center 2 ## 6212001674 @ 1285.xlsx')\n",
    "# achi = pd.read_excel('Standards/ACHI.XLSX')\n",
    "# service_name = get_service_name(data)\n",
    "# data_processed = create_document(service_name,0)\n",
    "# ACHI_processed = create_document(achi,3)\n",
    "# emb_achi = get_embeddings(ACHI_processed.iloc[:,3])\n",
    "# emb_data = get_embeddings(data_processed.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_similar(emb_data,emb_achi,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(data,tfdata,tfref):\n",
    "    result = pd.DataFrame(columns = ['Serv','cpt','Sim','Num'])\n",
    "    resultAll = pd.DataFrame(columns = ['Serv','cpt','Sim','Num'])\n",
    "    for i in range(len(data)):\n",
    "        for j in range(0,1):\n",
    "            index, simlarity = find_similar(tfdata, tfref, i, j)\n",
    "            result.loc[i] = [i, index, simlarity, j]\n",
    "            resultAll = resultAll.append(result)\n",
    "            result.drop(result.index, inplace = True)\n",
    "    #print(resultAll.head())\n",
    "    return resultAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = get_similarity(data,emb_data,emb_achi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.Sim.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_results(data,result_ACHI,result_LOINC,result_SFDA,result_DRS_ACHI,result_DRS_LOINC,result_DRS_SFDA):\n",
    "    df = pd.DataFrame(columns=['Serv','cpt','Sim','Num','MAPPED_CODE'])\n",
    "    for i in range(len(result_ACHI)):\n",
    "        #sims=[result_ACHI.iloc[i,2], result_LOINC.iloc[i,2]]\n",
    "        if data.iloc[i,5] != 'Lab' and data.iloc[i,5] != 'Other Medical Services':\n",
    "            if float(result_DRS_ACHI.iloc[i][2]) > 0.9 :\n",
    "                df.loc[i] = result_DRS_ACHI.iloc[i].to_list() + ['Drs_ACHI']\n",
    "            else:\n",
    "                df.loc[i] = result_ACHI.iloc[i].to_list() + ['ACHI']\n",
    "        elif data.iloc[i,5] == 'Other Medical Services':\n",
    "            if float(result_DRS_SFDA.iloc[i][2]) > 0.9 :\n",
    "                df.loc[i] = result_DRS_SFDA.iloc[i].to_list() + ['Drs_SFDA']\n",
    "            else:\n",
    "                df.loc[i] = result_SFDA.iloc[i].to_list() + ['SFDA']\n",
    "        elif data.iloc[i,5] == 'Lab':\n",
    "            if float(result_DRS_LOINC.iloc[i][2]) > 0.9 :\n",
    "                df.loc[i] = result_DRS_LOINC.iloc[i].to_list() + ['Drs_LOINC']\n",
    "            else:\n",
    "                df.loc[i] = result_LOINC.iloc[i].to_list() + ['LOINC']\n",
    "    #print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_map(data,result,ACHI,LOINC,SFDA,DRS_ACHI,DRS_LOINC,DRS_SFDA):\n",
    "    data['MAPPED_CODE'] = result['MAPPED_CODE']\n",
    "    for i in range(len(data)):\n",
    "        if result.iloc[i,4] == 'ACHI':\n",
    "            data.loc[i, 'UNIFIED_CODE'] = ACHI.iloc[int(result.iloc[i,1]),0]\n",
    "            data.loc[i, 'UNIFIED_CODE_DESCRIPTION'] = ACHI.iloc[int(result.iloc[i,1]),3]\n",
    "        elif result.iloc[i,4] == 'LOINC':\n",
    "            data.loc[i, 'UNIFIED_CODE'] = LOINC.iloc[int(result.iloc[i,1]),0]\n",
    "            data.loc[i, 'UNIFIED_CODE_DESCRIPTION'] = LOINC.iloc[int(result.iloc[i,1]),7]\n",
    "        elif result.iloc[i,4] == 'Drs_LOINC':\n",
    "            data.loc[i, 'UNIFIED_CODE'] = DRS_LOINC.iloc[int(result.iloc[i,1]),1]\n",
    "            data.loc[i, 'UNIFIED_CODE_DESCRIPTION'] = DRS_LOINC.iloc[int(result.iloc[i,1]),0]\n",
    "        elif result.iloc[i,4] == 'Drs_ACHI':\n",
    "            data.loc[i, 'UNIFIED_CODE'] = DRS_ACHI.iloc[int(result.iloc[i,1]),1]\n",
    "            data.loc[i, 'UNIFIED_CODE_DESCRIPTION'] = DRS_ACHI.iloc[int(result.iloc[i,1]),0]\n",
    "        elif result.iloc[i,4] == 'Drs_SFDA':\n",
    "            data.loc[i, 'UNIFIED_CODE'] = DRS_SFDA.iloc[int(result.iloc[i,1]),1]\n",
    "            data.loc[i, 'UNIFIED_CODE_DESCRIPTION'] = DRS_SFDA.iloc[int(result.iloc[i,1]),0]    \n",
    "        else:\n",
    "            data.loc[i, 'UNIFIED_CODE'] = SFDA.iloc[int(result.iloc[i,1]),0]\n",
    "            data.loc[i, 'UNIFIED_CODE_DESCRIPTION'] = SFDA.iloc[int(result.iloc[i,1]),3]\n",
    "    data['Similarity'] = result['Sim']\n",
    "    #print(data.head())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_similarity(data, thresh):\n",
    "    #data.loc[data['Similarity'] <= thresh, ['UNIFIED_CODE','UNIFIED_CODE_DESCRIPTION','MAPPED_CODE']] = \"Couldn't Be Mapped\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_columns(data):\n",
    "    columnsTitles = data.columns.to_list()\n",
    "    result = data.reindex(columns=columnsTitles[:5]+['UNIFIED_CODE_DESCRIPTION']+['UNIFIED_CODE']+['SERVICE_TYPE']+['MAPPED_CODE']+['Similarity'])\n",
    "    #print(result.head())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_mapping(data_path):\n",
    "    data = read_data(data_path)\n",
    "    print(\"raeding data finished\")\n",
    "    data = get_data_by_service_type(data, ['Lab','Diagnosis Procedure','Dental','Diagnostic Procedures','Other Medical Services','Package Deal','Physiotherapy','Radiology'])\n",
    "    print(\"filtering by service type finished\")\n",
    "    DRs_ACHI,DRs_LOINC,ACHI,DRs_SFDA,SFDA,LOINC = read_standards(get_standards_path())\n",
    "    print(\"reading standards finshed\")\n",
    "    service_name = get_service_name(data)\n",
    "    #print(service_name)\n",
    "    ACHI_processed = create_document(ACHI,3)\n",
    "    print(\"achi preprocessed\")\n",
    "    LOINC_processed = create_document(LOINC,7)\n",
    "    print(\"loinc preprocessed\")\n",
    "    SFDA_processed = create_document(SFDA,3)\n",
    "    print(\"sfda preprocessed\")\n",
    "    DRs_ACHI_processed = create_document(DRs_ACHI,0)\n",
    "    print(\"drs achi preprocessed\")\n",
    "    DRs_LOINC_processed = create_document(DRs_LOINC,0)\n",
    "    print(\"drs loinc preprocessed\")\n",
    "    DRs_SFDA_processed = create_document(DRs_SFDA,0)\n",
    "    print(\"drs sfda preprocessed\")\n",
    "    data_processed = create_document(service_name,0)\n",
    "    #print(data_processed)\n",
    "    print(\"data preprocessed\")\n",
    "    if len(data_processed) > 0:\n",
    "        emb_data = get_embeddings(data_processed.iloc[:,0])\n",
    "        print(\"data vectorized\")\n",
    "        emb_achi = get_embeddings(ACHI_processed.iloc[:,3])\n",
    "        print(\"achi vectorized\")\n",
    "        emb_loinc = get_embeddings(LOINC_processed.iloc[:,7])\n",
    "        print(\"loinc vectorized\")\n",
    "        emb_sfda = get_embeddings(SFDA_processed.iloc[:,3])\n",
    "        print(\"sfda vectorized\")\n",
    "        emb_drs_achi = get_embeddings(DRs_ACHI_processed.iloc[:,0])\n",
    "        print(\"drs achi vectorized\")\n",
    "        emb_drs_loinc = get_embeddings(DRs_LOINC_processed.iloc[:,0])\n",
    "        print(\"drs loinc vectorized\")\n",
    "        emb_drs_sfda = get_embeddings(DRs_SFDA_processed.iloc[:,0])\n",
    "        print(\"drs sfda vectorized\")\n",
    "        resultAll_ACHI = get_similarity(data,emb_data,emb_achi)\n",
    "        print(\"similarity for achi done\")\n",
    "        resultAll_LOINC = get_similarity(data,emb_data,emb_loinc)\n",
    "        print(\"similarity for loinc done\")\n",
    "        resultAll_SFDA = get_similarity(data,emb_data,emb_sfda)\n",
    "        print(\"similarity for sfda done\")\n",
    "        resultAll_DRS_ACHI = get_similarity(data,emb_data,emb_drs_achi)\n",
    "        print(\"similarity for drs achi done\")\n",
    "        resultAll_DRS_LOINC = get_similarity(data,emb_data,emb_drs_loinc)\n",
    "        print(\"similarity for drs loinc done\")\n",
    "        resultAll_DRS_SFDA = get_similarity(data,emb_data,emb_drs_sfda)\n",
    "        print(\"similarity for drs sfda done\")\n",
    "        AllResult = get_max_results(data,resultAll_ACHI,resultAll_LOINC,resultAll_SFDA,resultAll_DRS_ACHI,resultAll_DRS_LOINC,resultAll_DRS_SFDA)\n",
    "        print(\"getting max results done\")\n",
    "        AllResult = do_map(data,AllResult,ACHI,LOINC,SFDA,DRs_ACHI,DRs_LOINC,DRs_SFDA)\n",
    "        print(\"mapping done\")\n",
    "        #AllResult = filter_by_similarity(AllResult, 0.0)\n",
    "        AllResult = swap_columns(AllResult)\n",
    "    else:\n",
    "        AllResult = []\n",
    "        \n",
    "    return AllResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_excel(path):\n",
    "    writer = pd.ExcelWriter(path,engine='openpyxl')\n",
    "    writer.book = openpyxl.load_workbook(path)\n",
    "    \n",
    "    pandas.io.formats.excel.ExcelFormatter.header_style = None\n",
    "    \n",
    "    result[\"Y/N\"] = np.nan\n",
    "    result[\"Doctor's Recommended Code\"] = np.nan\n",
    "    result[\"Doctor's Recommended Code Description\"] = np.nan\n",
    "    result[(result.SERVICE_TYPE != 'Lab') & (result.SERVICE_TYPE != 'Other Medical Services')].to_excel(writer,sheet_name='ACHI',index=False)\n",
    "    result[result.SERVICE_TYPE == 'Lab'].to_excel(writer,sheet_name='LOINC',index=False)\n",
    "    result[result.SERVICE_TYPE == 'Other Medical Services'].to_excel(writer,sheet_name='SFDA',index=False)\n",
    "    \n",
    "    for i, sheetname in enumerate(writer.book.sheetnames):\n",
    "        worksheet = writer.book[sheetname]\n",
    "        mediumStyle = openpyxl.worksheet.table.TableStyleInfo(name='TableStyleMedium2',showRowStripes=True)\n",
    "        table = openpyxl.worksheet.table.Table(ref=worksheet.dimensions,displayName=\"Table\" + str(i),tableStyleInfo=mediumStyle)\n",
    "        worksheet.add_table(table)\n",
    "        \n",
    "        for col in worksheet.columns:\n",
    "            worksheet.column_dimensions[col[0].column_letter].width = (len(str(col[0].value)) + 2) * 1.3\n",
    "    \n",
    "    writer.book.save(path)\n",
    "    writer.book.close()\n",
    "    writer.close()\n",
    "    os.rename(path,os.path.join(\"mapped_embedding\",os.path.splitext(os.path.basename(path))[0]+\" Mapped_embedding\"+os.path.splitext(path)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started mapping Abeer Supreme Medical Center 2 ## 6212001674 @ 1285.xlsx\n",
      "raeding data finished\n",
      "filtering by service type finished\n",
      "reading standards finshed\n",
      "achi preprocessed\n",
      "loinc preprocessed\n",
      "sfda preprocessed\n",
      "drs achi preprocessed\n",
      "drs loinc preprocessed\n",
      "drs sfda preprocessed\n",
      "data preprocessed\n",
      "data vectorized\n",
      "achi vectorized\n",
      "loinc vectorized\n",
      "sfda vectorized\n",
      "drs achi vectorized\n",
      "drs loinc vectorized\n",
      "drs sfda vectorized\n",
      "similarity for achi done\n",
      "similarity for loinc done\n",
      "similarity for sfda done\n",
      "similarity for drs achi done\n",
      "similarity for drs loinc done\n",
      "similarity for drs sfda done\n",
      "getting max results done\n",
      "mapping done\n",
      "Finished mapping Abeer Supreme Medical Center 2 ## 6212001674 @ 1285.xlsx\n",
      "Exported Abeer Supreme Medical Center 2 ## 6212001674 @ 1285.xlsx to the 'mapped_embedding' folder\n",
      "Time: 00:12:14\n",
      "\n",
      "CPU times: user 15min 59s, sys: 2min 55s, total: 18min 54s\n",
      "Wall time: 12min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 1\n",
    "for provider_path in get_providers_path():\n",
    "    start_time = time.time()\n",
    "    print(\"Started mapping \" + os.path.basename(provider_path))\n",
    "    result = files_mapping(provider_path)\n",
    "    if len(result) > 0:\n",
    "        print(\"Finished mapping \" + os.path.basename(provider_path))\n",
    "        export_excel(provider_path)\n",
    "        print(\"Exported \" + os.path.basename(provider_path) + \" to the 'mapped_embedding' folder\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Time: \" + str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))) + \"\\n\")\n",
    "    else:\n",
    "        print(\"There is no mappings for this provider\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_excel('Providers/Shifa Jeddah Polyclinic ## 6212000304 @ 5903.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embed_sentences(pd.DataFrame(data.iloc[:4,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(data.iloc[:4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_vector = model.embed_sentence(sentence)\n",
    "#print(sentence_vector)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_vector1 = model.embed_sentence(preprocess_sentence('CKMB'))\n",
    "# sentence_vector2 = model.embed_sentence(preprocess_sentence('creatine kinase.MB\\creatine kinase.MB.total in serum or plasma by electrophoresis'))\n",
    "\n",
    "#cosine_sim = 1 - distance.cosine(sentence_vector1, sentence_vector2)\n",
    "# print('cosine similarity:', cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_similar(achi_emp.iloc[4],data_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embed_sentences(create_document(data,4)['SERVICE_NAME'])[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embed_sentences(create_document(data,4)['SERVICE_NAME'])[0].reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
