{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from nltk.stem import PorterStemmer\n",
    "# import pyodbc\n",
    "import collections\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROVIDER_CODE</th>\n",
       "      <th>SEGMENT_CODE</th>\n",
       "      <th>PROVIDER_NAME</th>\n",
       "      <th>SERVICE_CODE</th>\n",
       "      <th>SERVICE_NAME</th>\n",
       "      <th>SERVICE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>799102B</td>\n",
       "      <td>6212001674</td>\n",
       "      <td>Abeer Supreme Medical Center 2</td>\n",
       "      <td>1559-ENT</td>\n",
       "      <td>ARYTENOIDECTOMY</td>\n",
       "      <td>Package Deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>799102B</td>\n",
       "      <td>6212001674</td>\n",
       "      <td>Abeer Supreme Medical Center 2</td>\n",
       "      <td>1555-ENT</td>\n",
       "      <td>ORO ANTRAL FISTULA</td>\n",
       "      <td>Package Deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>799102B</td>\n",
       "      <td>6212001674</td>\n",
       "      <td>Abeer Supreme Medical Center 2</td>\n",
       "      <td>1556-ENT</td>\n",
       "      <td>TYMPANOPLASTY</td>\n",
       "      <td>Package Deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>799102B</td>\n",
       "      <td>6212001674</td>\n",
       "      <td>Abeer Supreme Medical Center 2</td>\n",
       "      <td>1557-ENT</td>\n",
       "      <td>STAPEDECTOMY</td>\n",
       "      <td>Package Deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>799102B</td>\n",
       "      <td>6212001674</td>\n",
       "      <td>Abeer Supreme Medical Center 2</td>\n",
       "      <td>1558-ENT</td>\n",
       "      <td>OPERATION ON VOCAL CORDS</td>\n",
       "      <td>Package Deal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PROVIDER_CODE  SEGMENT_CODE                   PROVIDER_NAME SERVICE_CODE  \\\n",
       "0       799102B    6212001674  Abeer Supreme Medical Center 2     1559-ENT   \n",
       "1       799102B    6212001674  Abeer Supreme Medical Center 2     1555-ENT   \n",
       "2       799102B    6212001674  Abeer Supreme Medical Center 2     1556-ENT   \n",
       "3       799102B    6212001674  Abeer Supreme Medical Center 2     1557-ENT   \n",
       "4       799102B    6212001674  Abeer Supreme Medical Center 2     1558-ENT   \n",
       "\n",
       "               SERVICE_NAME  SERVICE_TYPE  \n",
       "0           ARYTENOIDECTOMY  Package Deal  \n",
       "1        ORO ANTRAL FISTULA  Package Deal  \n",
       "2             TYMPANOPLASTY  Package Deal  \n",
       "3              STAPEDECTOMY  Package Deal  \n",
       "4  OPERATION ON VOCAL CORDS  Package Deal  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"Abeer Supreme Medical Center 2 ## 6212001674 @ 1285.xlsx\")\n",
    "data1 = data.copy()\n",
    "len(data1)\n",
    "data1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code_id</th>\n",
       "      <th>Block</th>\n",
       "      <th>Block Desc</th>\n",
       "      <th>ascii_desc</th>\n",
       "      <th>ascii_short_desc</th>\n",
       "      <th>effective_from</th>\n",
       "      <th>inactive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10801-00</td>\n",
       "      <td>1837</td>\n",
       "      <td>Speech audiometry</td>\n",
       "      <td>investigation evaluation fitting contact lenses</td>\n",
       "      <td>Investgtn evaln/fitting contact lenses</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>2000-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10816-00</td>\n",
       "      <td>1837</td>\n",
       "      <td>Speech audiometry</td>\n",
       "      <td>refitting contact lenses</td>\n",
       "      <td>Refitting of contact lenses</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>2000-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11000-00</td>\n",
       "      <td>1825</td>\n",
       "      <td>Electroencephalography [EEG]</td>\n",
       "      <td>electroencephalography</td>\n",
       "      <td>Electroencephalography</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11003-00</td>\n",
       "      <td>1825</td>\n",
       "      <td>Electroencephalography [EEG]</td>\n",
       "      <td>electroencephalography 3 hours duration</td>\n",
       "      <td>Electroencephalography of &gt;= 3 hours</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11006-00</td>\n",
       "      <td>1825</td>\n",
       "      <td>Electroencephalography [EEG]</td>\n",
       "      <td>temporosphenoidal electroencephalography</td>\n",
       "      <td>Temporosphenoidal electroencephalography</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7882</td>\n",
       "      <td>97981-00</td>\n",
       "      <td>490</td>\n",
       "      <td>Miscellaneous dental services</td>\n",
       "      <td>splinting stabilisation tooth direct</td>\n",
       "      <td>Splint &amp; stabilisation of tooth, direct</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7883</td>\n",
       "      <td>97982-00</td>\n",
       "      <td>490</td>\n",
       "      <td>Miscellaneous dental services</td>\n",
       "      <td>enamel stripping tooth</td>\n",
       "      <td>Enamel stripping of tooth</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7884</td>\n",
       "      <td>97983-00</td>\n",
       "      <td>490</td>\n",
       "      <td>Miscellaneous dental services</td>\n",
       "      <td>intravenous cannulation establishment infusion...</td>\n",
       "      <td>IV cannulation &amp; est infusion by dentist</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>2000-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7885</td>\n",
       "      <td>97985-00</td>\n",
       "      <td>490</td>\n",
       "      <td>Miscellaneous dental services</td>\n",
       "      <td>provision oral appliance diagnosed snoring obs...</td>\n",
       "      <td>Prov oral appliance for snoring &amp; OSA</td>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7886</td>\n",
       "      <td>97986-00</td>\n",
       "      <td>490</td>\n",
       "      <td>Miscellaneous dental services</td>\n",
       "      <td>postoperative dental care elsewhere classified</td>\n",
       "      <td>Postoperative dental care, NEC</td>\n",
       "      <td>1998-07-01</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7887 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Code_id  Block                     Block Desc  \\\n",
       "0     10801-00   1837              Speech audiometry   \n",
       "1     10816-00   1837              Speech audiometry   \n",
       "2     11000-00   1825   Electroencephalography [EEG]   \n",
       "3     11003-00   1825   Electroencephalography [EEG]   \n",
       "4     11006-00   1825   Electroencephalography [EEG]   \n",
       "...        ...    ...                            ...   \n",
       "7882  97981-00    490  Miscellaneous dental services   \n",
       "7883  97982-00    490  Miscellaneous dental services   \n",
       "7884  97983-00    490  Miscellaneous dental services   \n",
       "7885  97985-00    490  Miscellaneous dental services   \n",
       "7886  97986-00    490  Miscellaneous dental services   \n",
       "\n",
       "                                             ascii_desc  \\\n",
       "0       investigation evaluation fitting contact lenses   \n",
       "1                              refitting contact lenses   \n",
       "2                                electroencephalography   \n",
       "3               electroencephalography 3 hours duration   \n",
       "4              temporosphenoidal electroencephalography   \n",
       "...                                                 ...   \n",
       "7882               splinting stabilisation tooth direct   \n",
       "7883                             enamel stripping tooth   \n",
       "7884  intravenous cannulation establishment infusion...   \n",
       "7885  provision oral appliance diagnosed snoring obs...   \n",
       "7886     postoperative dental care elsewhere classified   \n",
       "\n",
       "                              ascii_short_desc effective_from   inactive  \n",
       "0       Investgtn evaln/fitting contact lenses     1998-07-01 2000-07-01  \n",
       "1                  Refitting of contact lenses     1998-07-01 2000-07-01  \n",
       "2                       Electroencephalography     1998-07-01        NaT  \n",
       "3         Electroencephalography of >= 3 hours     1998-07-01        NaT  \n",
       "4     Temporosphenoidal electroencephalography     1998-07-01        NaT  \n",
       "...                                        ...            ...        ...  \n",
       "7882   Splint & stabilisation of tooth, direct     1998-07-01        NaT  \n",
       "7883                 Enamel stripping of tooth     1998-07-01        NaT  \n",
       "7884  IV cannulation & est infusion by dentist     1998-07-01 2000-07-01  \n",
       "7885     Prov oral appliance for snoring & OSA     2000-07-01        NaT  \n",
       "7886            Postoperative dental care, NEC     1998-07-01        NaT  \n",
       "\n",
       "[7887 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_document(data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SERVICE_NAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SERVICE_NAME'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9088d650e6d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SERVICE_NAME'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DAY PKG'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SERVICE_NAME'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DAYS'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(len(ref1[ref1['LongDescription'].str.contains('CONSULTATION')]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SERVICE_NAME'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(len(ref1[ref1['LongDescription'].isna()]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SERVICE_NAME'"
     ]
    }
   ],
   "source": [
    "print(len(data1[data1['SERVICE_NAME'].str.contains('DAY PKG')]))\n",
    "print(len(data1[data1['SERVICE_NAME'].str.contains('DAYS')]))\n",
    "#print(len(ref1[ref1['LongDescription'].str.contains('CONSULTATION')]))\n",
    "print(len(data1[data1['SERVICE_NAME'].isna()]))\n",
    "#print(len(ref1[ref1['LongDescription'].isna()]))\n",
    "#ref[ref['LongDescription'].str.contains('Scan')]\n",
    "#ref[ref['LongDescription'].str.contains('CT')]\n",
    "#ref[ref['LongDescription'].str.contains('C.T')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['SERVICE_NAME'] = data1['SERVICE_NAME'].str.replace('DAY PKG', '')\n",
    "data1['SERVICE_NAME'] = data1['SERVICE_NAME'].str.replace('DAYS', '')\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = TfidfVectorizer()\n",
    "data1['SERVICE_NAME'] = data1['SERVICE_NAME'].apply(lambda x: ''.join(i for i in x if not i.isdigit()))\n",
    "tf1.fit_transform(data1['SERVICE_NAME'])\n",
    "print(len(tf1.get_feature_names()))\n",
    "data_features = pd.DataFrame(tf1.get_feature_names(), columns = ['Features'])\n",
    "data_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_name = pd.DataFrame(data1['SERVICE_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACHI = pd.read_excel('ACHI.XLSX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACHI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(DF1,col_index): #DF1 is the DF to be preprocessed #col_index is the number of the describtion column (int)\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    uselessWords = ['intended','for','use','helps','to','provide','all','detection','of']\n",
    "    # Making A copy to prevent change to the original DF as it is needd subsequently\n",
    "    DF = DF1.copy()\n",
    "    # Insuring string type for the wanted col\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].astype(str)\n",
    "    # Removing all newline and replacing it with space\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].str.replace('\\n',' ')\n",
    "    #Replacing all non alphanumeric in both english and arabic by space\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "    #Separating camel case with space\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", x))\n",
    "    #lowercasing all describtions\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "    #removing stop words\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x:\" \".join (word for word in x.split() if word not in stopWords))\n",
    "    #removing useless words\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x:\" \".join(word for word in x.split() if word not in uselessWords))\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x:''.join(i for i in x if not i.isdigit()))\n",
    "    #In case a med dict present (referring the medical abbreviation to its origin) this line return the abbreviation to origin\n",
    "    ## DF_document.document = DF_document.document.apply(lambda x: \" \".join(med_dict(word) for word in x.split()) )\n",
    "    #Counting all Words that occur either in very low freq or very high and removing them (Threshhold must be configured)\n",
    "    ## freq = pd.Series(' '.join(DF[DF.columns[col_index]]).split()).value_counts()[:50]\n",
    "    ## DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: \" \".join(x for x in x.split() if  x not in freq.index))\n",
    "    ## freq = pd.Series(' '.join(DF[DF.columns[col_index]]).split()).value_counts()[-1034:]\n",
    "    ## DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: \" \".join(x for x in x.split() if x not in freq.index))\n",
    "    #Defining a stemmer and stemming all words in the description\n",
    "    #st = PorterStemmer()\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "    #DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: \"\" if x.isdigit() == True else x)\n",
    "    return DF\n",
    "# create_document(data,4).to_excel('haha.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function that take a single row string as first argument and the DF to be looked upon \n",
    "#this function return the index of service which has an exact match in the lookup or Null if there is not one\n",
    "def dup_row_refrence(sd_desc,unique_list):\n",
    "    for index,value in unique_list.items():\n",
    "        if sd_desc == value:\n",
    "            return index\n",
    "    return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that uses dup_row_refrence by taking 1 DF and refrencing it by its self\n",
    "def dup_dup(DF_dup, col_name):\n",
    "    #Dropping duplicate from DF to create a refrence (drop duplicate except First)\n",
    "    DF_unique =  DF_dup[col_name].drop_duplicates()\n",
    "    #getting the subset of the services of processed DF that occur in refrence (all except first also)\n",
    "    DF_dup = DF_dup[DF_dup.duplicated(subset = col_name)]\n",
    "    #applying dup_row_refrence for each row\n",
    "    dup_ref = DF_dup[col_name].apply(lambda x: dup_row_refrence(x,DF_unique))\n",
    "    return dup_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that uses dup_row_refrence by taking 2 DF and refrencing one by the other\n",
    "def dup_ref(DF_ref,DF_dup):\n",
    "    #Dropping duplicate from refrence\n",
    "    DF_unique =  DF_ref[\"ascii_desc\"].drop_duplicates()\n",
    "    #getting the subset of the services of processed DF that occur in refrence\n",
    "    DF_dup = DF_dup[DF_dup[\"SERVICE_NAME\"].isin(DF_unique)]\n",
    "    #applying dup_row_refrence for each row\n",
    "    dup_ref = DF_dup['SERVICE_NAME'].apply(lambda x: dup_row_refrence(x,DF_unique))\n",
    "    return dup_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACHI_processed = create_document(ACHI, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACHI_processed[ACHI_processed.columns[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ACHI_processed))\n",
    "ACHI_processed['ascii_desc'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfobj = TfidfVectorizer()\n",
    "tf = tfobj.fit_transform(ACHI_processed['ascii_desc'])\n",
    "ACHI_features = pd.DataFrame(tfobj.get_feature_names(), columns=['Features'])\n",
    "ACHI_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_features[data_features['Features'].isin(ACHI_features['Features']) == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that uses dup_row_refrence by taking 2 DF and refrencing one by the other\n",
    "def dup_ref_ACHI(DF_ref,DF_dup):\n",
    "    #Dropping duplicate from refrence\n",
    "    DF_unique =  DF_ref[\"ascii_desc\"].drop_duplicates()\n",
    "    #getting the subset of the services of processed DF that occur in refrence\n",
    "    DF_dup = DF_dup[DF_dup[\"SERVICE_NAME\"].isin(DF_unique)]\n",
    "    #applying dup_row_refrence for each row\n",
    "    dup_ref = DF_dup['SERVICE_NAME'].apply(lambda x: dup_row_refrence(x,DF_unique))\n",
    "    return dup_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = create_document(data1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_ref_ACHI(ACHI_processed, data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACHI_processed_ = dup_dup(ACHI_processed, 'ascii_desc')\n",
    "print('len of duplicated services --> {}'.format(len(ACHI_processed_)))\n",
    "print(len(collections.Counter(ACHI_processed['ascii_desc'])))\n",
    "ACHI_processed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_data = pd.DataFrame()\n",
    "DF_ref = pd.DataFrame()\n",
    "DF_data['Name'] = data_processed['SERVICE_NAME']\n",
    "DF_ref['Name'] = ACHI_processed['ascii_desc']\n",
    "DF = pd.concat(objs = [DF_data,DF_ref])\n",
    "DF['Name'] = DF['Name'].apply(lambda x: ''.join(i for i in x if not i.isdigit()))\n",
    "len(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "tfidf = TfidfVectorizer(stop_words=stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.fit(DF['Name'])\n",
    "tfref = tfidf.transform(ACHI_processed['ascii_desc'])\n",
    "tfdata = tfidf.transform(data_processed['SERVICE_NAME'])\n",
    "print(tfdata.shape)\n",
    "print(tfref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function finds the (rank) most similar description from 1 tfidf matrixin another and return it\n",
    "def find_similar(tfidf_data,tfidf_ref, index, rank = 0):\n",
    "    #multiplying the 2 matrix to find the cosine similarity\n",
    "    cosine_similarities = linear_kernel(tfidf_data[index:index+1], tfidf_ref[:]).flatten()\n",
    "    #getting the most similar index\n",
    "    highest_ind = cosine_similarities.argsort()[::-1][rank]\n",
    "    #return the index along with its similarity\n",
    "    return highest_ind, cosine_similarities[highest_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = pd.DataFrame(columns = ['Serv','cpt','Sim','Num'])\n",
    "resultAll = pd.DataFrame(columns = ['Serv','cpt','Sim','Num'])\n",
    "for i in range(len(data_processed)):\n",
    "    for j in range(0,3):\n",
    "        index, simlarity = find_similar(tfdata, tfref, i, j)\n",
    "        result.loc[i] = [i, index, simlarity, j]\n",
    "        resultAll = resultAll.append(result)\n",
    "        result.drop(result.index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.iloc[1,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resultAll[resultAll['Sim'] > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = resultAll.copy()\n",
    "result['Serv'] = result['Serv'].astype(int)\n",
    "result['cpt'] = result['cpt'].astype(int)\n",
    "result['Num'] = result['Num'].astype(int)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pred = pd.DataFrame(columns=['Serv','cpt','Sim','Num'])\n",
    "second_pred = pd.DataFrame(columns=['Serv','cpt','Sim','Num'])\n",
    "third_pred = pd.DataFrame(columns=['Serv','cpt','Sim','Num'])\n",
    "\n",
    "first_pred = result[result['Num'] == 0]\n",
    "second_pred = result[result['Num'] == 1]\n",
    "third_pred = result[result['Num'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pred.columns = ['Serv', 'ACHI_1','Sim_1','Num']\n",
    "second_pred.columns = ['Serv', 'ACHI_2','Sim_2','Num']\n",
    "third_pred.columns = ['Serv', 'ACHI_3','Sim_3','Num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = pd.concat(objs = [first_pred,second_pred, third_pred], axis=1, sort=False)\n",
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult.drop(['Num'], axis=1, inplace=True)\n",
    "AllResult.columns = ['Al Rajhi Service','ACHI_1','Sim_1','Serv','ACHI_2','Sim_2','Serv','ACHI_3','Sim_3']\n",
    "AllResult.drop(['Serv'], axis=1, inplace=True)\n",
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = AllResult.set_index('Al Rajhi Service')\n",
    "AllResult = pd.concat(objs = [AllResult, data[['PROVIDER_CODE','SEGMENT_CODE','PROVIDER_NAME','SERVICE_CODE','SERVICE_NAME','SERVICE_TYPE']]],axis = 1 , join_axes=[AllResult.index] )\n",
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = AllResult.reset_index()\n",
    "AllResult = AllResult.set_index('ACHI_1')\n",
    "AllResult = pd.concat(objs = [AllResult, ACHI[['ascii_short_desc','Code_id','Block']]], axis = 1, join_axes=[AllResult.index])\n",
    "AllResult = AllResult.reset_index()\n",
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult.drop(['Al Rajhi Service'], axis=1, inplace=True)\n",
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult.drop(['ACHI_1'],axis =1 ,inplace=True)\n",
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult.columns = ['Score_1','ACHI_2','Score_2','ACHI_3','Score_3','PROVIDER_CODE','SEGMENT_CODE','PROVIDER_NAME','SERVICE_CODE','SERVICE_NAME','SERVICE_TYPE','ACHI_1','Code_id_1','Block_1']\n",
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult['SCORE'] = AllResult['Score_1']\n",
    "AllResult.drop(['Score_1'], axis=1, inplace=True)\n",
    "AllResult.columns = ['ACHI_2','Score_2','ACHI_3','Score_3','PROVIDER_CODE','SEGMENT_CODE','PROVIDER_NAME','SERVICE_CODE','SERVICE_NAME','SERVICE_TYPE','ACHI_1','MBS_1','Block_1','Score_1']\n",
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = AllResult.set_index('ACHI_2')\n",
    "AllResult = pd.concat(objs = [AllResult,ACHI[['ascii_desc','Code_id','Block']] ], axis = 1, join_axes=[AllResult.index])\n",
    "AllResult.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = AllResult.reset_index()\n",
    "AllResult['SCORE'] = AllResult['Score_2']\n",
    "AllResult.drop(['ACHI_2','Score_2'], axis=1, inplace=True)\n",
    "AllResult.columns = ['ACHI_3','Score_3','PROVIDER_CODE','SEGMENT_CODE','PROVIDER_NAME','SERVICE_CODE','SERVICE_NAME','SERVICE_TYPE','ACHI_1','MBS_1','Block_1','Score_1','ACHI_2','Code_id_2','Block_2','Score_2']\n",
    "AllResult.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = AllResult.set_index('ACHI_3')\n",
    "AllResult = pd.concat(objs = [AllResult, ACHI[['ascii_desc','Code_id','Block']] ], axis = 1, join_axes=[AllResult.index])\n",
    "AllResult.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = AllResult.reset_index()\n",
    "AllResult['SCORE'] = AllResult['Score_3']\n",
    "AllResult.drop(['ACHI_3','Score_3'], axis=1, inplace=True)\n",
    "AllResult.columns = ['PROVIDER_CODE','SEGMENT_CODE','PROVIDER_NAME','SERVICE_CODE','SERVICE_NAME','SERVICE_TYPE','ACHI_1','Code_id_1','Block_1','Score_1','ACHI_2','Code_id_2','Block_2','Score_2','ACHI_3','Code_id_3','Block_3','Score_3']\n",
    "AllResult.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = AllResult.sort_values(by = ['Score_1','Score_2','Score_3'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(AllResult[AllResult.Score_1 > 0.5])/len(AllResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(DF,col_index):\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].astype(str)\n",
    "    DF[DF.columns[col_index]] = DF[DF.columns[col_index]].apply(lambda x: x[3:] if x[:3] =='- -' else x)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult_processed = parse(AllResult,3)\n",
    "AllResult_processed = parse(AllResult_processed,7)\n",
    "AllResult_processed = parse(AllResult_processed,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult_processed.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult_processed.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_index(x):\n",
    "#     return service_name['Service Name'][service_name['Service Name'] == str(x)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_similar(l1,l2,description):\n",
    "#     l3 = []\n",
    "#     for i in l1:\n",
    "#         l3.append(description.values[np.argmax([cosine_similarity(i,j) for j in l2])][0])\n",
    "#     return l3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_similar(tfidf_data,tfidf_ref, index, rank = 1):\n",
    "#     #multiplying the 2 matrix to find the cosine similarity\n",
    "#     cosine_similarities = linear_kernel(tfidf_data[index:index+1], tfidf_ref[:]).flatten()\n",
    "#     #getting the most similar index\n",
    "#     highest_ind = cosine_similarities.argsort()[::-1][rank]\n",
    "#     #return the index along with its similarity\n",
    "#     return highest_ind, cosine_similarities[highest_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_name.iloc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pd.DataFrame(columns = ['Serv','achi','Sim'])\n",
    "# resultAll = pd.DataFrame(columns = ['Serv','achi','Sim'])\n",
    "# for i in range(len(service_name)):\n",
    "#     index, simlarity = find_similar(sn_vect, desc_vectorized, i, 1)\n",
    "#     result.loc[i] = [service_name.iloc[i][0], achi.iloc[index][1], simlarity]\n",
    "#     resultAll = resultAll.append(result)\n",
    "#     result.drop(result.index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultAll.columns = ['Service Name', 'Achi','Similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_similar(sn_vect,desc_vectorized,description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['ACHI'].apply(lambda x : description.iloc[np.argmax([cosine_similarity(sn_vect[get_index(x)],j) for j in desc_vectorized]),0]).to_csv(r'D:\\\\Users\\\\Testing\\\\Desktop\\\\al-rajhai\\\\result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
